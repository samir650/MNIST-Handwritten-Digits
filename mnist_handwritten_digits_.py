# -*- coding: utf-8 -*-
"""MNIST Handwritten Digits .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Rajggtrw59scd8Uh57zhMTzZaOT3I3S

# **MINST Dataset**

# import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import keras
from keras import layers, datasets, models
from skimage.io import imread,imshow,imsave
import skimage
import cv2

"""# Read Data"""

(x_train,y_train) , (x_test,y_test) = datasets.mnist.load_data()

x_train.shape

"""# Checking train dataset"""

fig = plt.figure(figsize=(25, 4))
for idx in np.arange(10):
    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])
    ax.imshow(x_train[idx], cmap='Blues_r')
    ax.set_title(str(y_train[idx]),fontsize=25)

np.unique(y_train, return_counts=True)

sns.heatmap(x_train[0], cmap='Blues_r')

"""# Data Preprocessing

### Normalize the pixel values of the images to be in the range [0, 1]
"""

x_train = x_train.astype('float32')/255
x_test = x_test.astype('float32')/255

sns.heatmap(x_train[0], cmap='Blues_r')

"""### Reshape the input images"""

# Reshape the images to add a channel dimension (for Conv2D)
x_train = x_train.reshape((-1, 28, 28, 1))
x_test = x_test.reshape((-1, 28, 28, 1))

"""### One Hot Encoding"""

num_classes = 10
y_train = keras.utils.to_categorical(y_train,num_classes)
y_test = keras.utils.to_categorical(y_test,num_classes)

y_train

plt.figure(figsize=(20,20))

for n,i in enumerate(list(np.random.randint(0,len(x_train),36))):
  plt.subplot(6,6,n+1)
  plt.imshow(x_train[i], cmap='gray')
  plt.axis('off')

"""# Build Model

### Model Architecture (LeNet-5)
"""

model = models.Sequential([
    layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(28, 28, 1)),
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),
    layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh'),
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),
    layers.Flatten(),
    layers.Dense(120, activation='tanh'),
    layers.Dense(84, activation='tanh'),
    layers.Dense(10, activation='softmax')
])

model.summary()

keras.utils.plot_model(model, show_shapes=True)

"""### Compiles the model"""

model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])

"""### Model Traning"""

training_history = model.fit(x_train,y_train, epochs = 15, validation_split= 0.1 , batch_size = 128 )

"""### Model Evaluation on Test set"""

score_acc = model.evaluate(x_test,y_test)
score_acc

print('Test loss', score_acc[0])
print('Test Accuaracy', score_acc[1])

plt.plot(training_history.history['loss'], label = 'training loss')
plt.plot(training_history.history['val_loss'], label = 'val loss')
plt.legend()

plt.plot(training_history.history['accuracy'], label = 'training set')
plt.plot(training_history.history['val_accuracy'], label = 'val set')
plt.legend()

plt.imshow(x_test[87].reshape(28,28))
test_value = model.predict(x_test[87].reshape(1,28,28,1))
test_value = test_value.argmax()
print('predict value ', test_value)

predictions = model.predict(x_test)
predictions = np.argmax(predictions,axis = 1)

import seaborn as sns
import tensorflow as tf

confusion_matrix = tf.math.confusion_matrix(np.argmax(y_test, axis=1), predictions)
f, ax = plt.subplots(figsize=(9, 7))
sns.heatmap(
    confusion_matrix,
    annot=True,
    linewidths=.5,
    fmt="d",
    square=True,
    ax=ax
)
plt.show()

"""# Save the model"""

import joblib

# Save the model
joblib.dump(model, 'CNN_model.joblib')